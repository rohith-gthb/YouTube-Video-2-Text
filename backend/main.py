import re
import asyncio
import json
import os
import random
import requests
import zipfile
import io
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from fastapi import FastAPI, HTTPException, Query, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound
from dotenv import load_dotenv
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
from ExtractUrls import extract_channel_videos
import traceback


# List of common browser User-Agents for rotation
USER_AGENTS = [
    # Chrome on Windows 10
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    
    # Chrome on macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
    
    # Firefox on Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0",
    
    # Firefox on macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:124.0) Gecko/20100101 Firefox/124.0",
    
    # Safari on macOS
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4 Safari/605.1.15",
    
    # Edge on Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 Edg/123.0.0.0"
]



def get_random_headers():
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept-Language": "en-US,en;q=0.9",
    }

# Load environment variables
load_dotenv()

# Setup Rate Limiting
limiter = Limiter(key_func=get_remote_address)
app = FastAPI(title="YouTube Transcript API")

# Initialize API once
ytt_api = YouTubeTranscriptApi()

# Cookie file path
COOKIE_FILE = os.path.join(os.path.dirname(__file__), "cookies.txt")

def get_cookies():
    """Returns the cookie file path if it exists, otherwise None."""
    if os.path.exists(COOKIE_FILE):
        return COOKIE_FILE
    return None

def format_time(seconds: float) -> str:
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    return f"{str(h) + ':' if h > 0 else ''}{str(m).zfill(2)}:{str(s).zfill(2)}"

def clean_transcript_text(text: str) -> str:
    # Robust cleaning: Remove '>>', '>>>', [Music], (Laughter), etc.
    text = text.replace("&gt;&gt;", "").replace("&gt;", "")
    text = re.sub(r'>>+', '', text) 
    text = re.sub(r'\[.*?\]', '', text) 
    text = re.sub(r'\(.*?\)', '', text) 
    return text.strip()

def get_formatted_content(video_title: str, video_url: str, segments: List[Dict], format_type: str, include_timestamps: bool) -> str:
    """
    Unified formatter for all export types.
    """
    if format_type == 'json':
        return json.dumps({
            "title": video_title,
            "url": video_url,
            "transcript": segments
        }, indent=2)

    if format_type == 'csv':
        headers = "Start,Duration,Text" if include_timestamps else "Sentence"
        rows = []
        if include_timestamps:
            for s in segments:
                clean_text = s['text'].replace('"', '""')
                rows.append(f"{s['start']},{s['duration']},\"{clean_text}\"")
        else:
            full_text = " ".join([s['text'] for s in segments])
            sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', full_text) if s.strip()]
            for s in sentences:
                clean_sentence = s.replace('"', '""')
                rows.append(f"\"{clean_sentence}\"")
        return f"{headers}\n" + "\n".join(rows)

    if format_type == 'md':
        content = f"# {video_title}\n\nURL: {video_url}\n\nGenerated by Transcribe.Pro\n\n"
        if include_timestamps:
            for s in segments:
                content += f"**{format_time(s['start'])}**: {s['text']}\n\n"
        else:
            full_text = " ".join([s['text'] for s in segments])
            sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', full_text) if s.strip()]
            content += "\n\n".join([f"> {s}" for s in sentences])
        return content

    # Default to TXT
    if include_timestamps:
        return "\n".join([f"[{format_time(s['start'])}] {s['text']}" for s in segments])
    else:
        full_text = " ".join([s['text'] for s in segments])
        sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', full_text) if s.strip()]
        return "\n".join(sentences)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# Environment-based CORS
ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "*").split(",")

# Enable CORS for frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def extract_video_id(url: str) -> Optional[str]:
    """Extracts the video ID from a YouTube URL."""
    patterns = [
        r"(?:v=|\/)([0-9A-Za-z_-]{11}).*",
        r"youtu\.be\/([0-9A-Za-z_-]{11})",
        r"embed\/([0-9A-Za-z_-]{11})",
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return None

def get_video_info(video_id: str):
    """Fetches video metadata using requests and BeautifulSoup (Safer Alternative)."""
    url = f"https://www.youtube.com/watch?v={video_id}"
    try:
        response = requests.get(url, headers=get_random_headers(), timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extract title
        title = ""
        title_tag = soup.find("meta", property="og:title")
        if title_tag:
            title = title_tag["content"]
        
        # Extract thumbnail
        thumbnail = ""
        thumbnail_tag = soup.find("meta", property="og:image")
        if thumbnail_tag:
            thumbnail = thumbnail_tag["content"]
            
        # Extract author
        author = ""
        author_tag = soup.find("link", itemprop="name")
        if author_tag:
            author = author_tag["content"]
        elif soup.find("span", itemprop="author"):
            author = soup.find("span", itemprop="author").find("link", itemprop="name")["content"]

        return {
            "title": title or "Unknown Title",
            "thumbnail": thumbnail or f"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg",
            "author": author or "Unknown Author",
        }
    except Exception as e:
        print(f"Error fetching metadata: {e}")
        return None

@app.get("/transcript")
@limiter.limit("5/minute")
async def get_transcript(request: Request, url: str = Query(..., description="The YouTube video URL")):
    video_id = extract_video_id(url)
    if not video_id:
        raise HTTPException(status_code=400, detail="Invalid YouTube URL")

    video_info = get_video_info(video_id)
    if not video_info:
        # Fallback to basic info if scraping fails but video ID is valid
        video_info = {
            "title": "YouTube Video",
            "thumbnail": f"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg",
            "author": "YouTube"
        }

    try:
        cookies = get_cookies()
        transcript_list = ytt_api.fetch(video_id, cookies=cookies) if cookies else ytt_api.fetch(video_id)
        
        formatted_transcript = []
        full_text = ""
        for entry in transcript_list:
            clean_text = clean_transcript_text(entry.text)
            if not clean_text:
                continue
                
            formatted_transcript.append({
                "text": clean_text,
                "start": entry.start,
                "duration": entry.duration
            })
            full_text += clean_text + " "

        # Backend Sentence Processing
        full_text = full_text.strip()
        # Split by . ! ? while keeping the delimiters, then group into logical lines
        sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', full_text) if s.strip()]

        return {
            "video_id": video_id,
            "info": video_info,
            "transcript": formatted_transcript,
            "full_text": full_text,
            "sentences": sentences
        }
    except TranscriptsDisabled:
        raise HTTPException(status_code=404, detail="Transcripts are disabled for this video")
    except NoTranscriptFound:
        raise HTTPException(status_code=404, detail="No transcript found for this video")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")

@app.get("/channel/videos")
@limiter.limit("5/minute")
async def get_channel_videos(request: Request, url: str = Query(..., description="The YouTube channel URL")):
    """
    Returns a list of videos in a channel.
    """
    videos = extract_channel_videos(url)
    if not videos:
        raise HTTPException(status_code=404, detail="No videos found or unable to fetch channel.")
    return {"videos": videos}

@app.post("/channel/zip")
@limiter.limit("2/minute")
async def get_channel_zip(
    request: Request,
    data: Dict = None,
    format: str = Query("txt", description="The export format (txt, csv, md, json)"),
    include_timestamps: bool = Query(True, description="Whether to include timestamps")
):
    """
    Fetches transcripts for a specific list of video URLs and returns them as a ZIP file.
    """
    if not data or 'videos' not in data:
        raise HTTPException(status_code=400, detail="Missing video list.")

    videos = data['videos'] # Expecting [{"title": "...", "url": "..."}, ...]

    if not videos:
        raise HTTPException(status_code=400, detail="Video list is empty.")

    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, "a", zipfile.ZIP_DEFLATED, False) as zip_file:
        cookies = get_cookies()
        for video in videos:
            video_id = video['url'].split('=')[-1]
            try:
                # Add a small delay to avoid IP blocking
                await asyncio.sleep(0.5) 
                transcript_list = ytt_api.fetch(video_id, cookies=cookies) if cookies else ytt_api.fetch(video_id)
                
                cleaned_segments = []
                for entry in transcript_list:
                    clean_text = clean_transcript_text(entry.text)
                    if clean_text:
                        cleaned_segments.append({
                            "text": clean_text,
                            "start": entry.start,
                            "duration": entry.duration
                        })
                
                if not cleaned_segments:
                    continue

                content = get_formatted_content(
                    video['title'], 
                    video['url'], 
                    cleaned_segments, 
                    format, 
                    include_timestamps
                )
                
                safe_title = "".join([c for c in video['title'] if c.isalnum() or c in (' ', '-', '_')]).strip()
                safe_title = safe_title[:50] or video_id
                
                zip_file.writestr(f"{safe_title}.{format}", content)
            except Exception:
                continue

    if len(zip_buffer.getvalue()) < 100:
        raise HTTPException(status_code=404, detail="No transcripts could be extracted.")

    zip_buffer.seek(0)
    return StreamingResponse(
        zip_buffer,
        media_type="application/x-zip-compressed",
        headers={"Content-Disposition": f"attachment; filename=selected_transcripts_{format}.zip"}
    )

@app.get("/health")
async def health_check():
    return {"status": "ok"}

if __name__ == "__main__":
    import uvicorn
    # Enable reload for development
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
